{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alden/quantfin/lib/python3.11/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1978-01-03</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.150002</td>\n",
       "      <td>93.489998</td>\n",
       "      <td>93.820000</td>\n",
       "      <td>93.820000</td>\n",
       "      <td>17720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978-01-04</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.099998</td>\n",
       "      <td>92.570000</td>\n",
       "      <td>93.519997</td>\n",
       "      <td>93.519997</td>\n",
       "      <td>24090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978-01-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.529999</td>\n",
       "      <td>92.510002</td>\n",
       "      <td>92.739998</td>\n",
       "      <td>92.739998</td>\n",
       "      <td>23570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978-01-06</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.660004</td>\n",
       "      <td>91.050003</td>\n",
       "      <td>91.620003</td>\n",
       "      <td>91.620003</td>\n",
       "      <td>26150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978-01-09</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.480003</td>\n",
       "      <td>89.970001</td>\n",
       "      <td>90.639999</td>\n",
       "      <td>90.639999</td>\n",
       "      <td>27990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-05</th>\n",
       "      <td>5110.520020</td>\n",
       "      <td>5114.540039</td>\n",
       "      <td>5056.819824</td>\n",
       "      <td>5078.649902</td>\n",
       "      <td>5078.649902</td>\n",
       "      <td>4418410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-06</th>\n",
       "      <td>5108.029785</td>\n",
       "      <td>5127.970215</td>\n",
       "      <td>5092.220215</td>\n",
       "      <td>5104.759766</td>\n",
       "      <td>5104.759766</td>\n",
       "      <td>4559050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-07</th>\n",
       "      <td>5132.379883</td>\n",
       "      <td>5165.620117</td>\n",
       "      <td>5128.209961</td>\n",
       "      <td>5157.359863</td>\n",
       "      <td>5157.359863</td>\n",
       "      <td>4137980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-08</th>\n",
       "      <td>5164.459961</td>\n",
       "      <td>5189.259766</td>\n",
       "      <td>5117.500000</td>\n",
       "      <td>5123.689941</td>\n",
       "      <td>5123.689941</td>\n",
       "      <td>4208870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-11</th>\n",
       "      <td>5111.959961</td>\n",
       "      <td>5115.439941</td>\n",
       "      <td>5106.689941</td>\n",
       "      <td>5111.609863</td>\n",
       "      <td>5111.609863</td>\n",
       "      <td>167092004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11646 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "1978-01-03     0.000000    95.150002    93.489998    93.820000    93.820000   \n",
       "1978-01-04     0.000000    94.099998    92.570000    93.519997    93.519997   \n",
       "1978-01-05     0.000000    94.529999    92.510002    92.739998    92.739998   \n",
       "1978-01-06     0.000000    92.660004    91.050003    91.620003    91.620003   \n",
       "1978-01-09     0.000000    91.480003    89.970001    90.639999    90.639999   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2024-03-05  5110.520020  5114.540039  5056.819824  5078.649902  5078.649902   \n",
       "2024-03-06  5108.029785  5127.970215  5092.220215  5104.759766  5104.759766   \n",
       "2024-03-07  5132.379883  5165.620117  5128.209961  5157.359863  5157.359863   \n",
       "2024-03-08  5164.459961  5189.259766  5117.500000  5123.689941  5123.689941   \n",
       "2024-03-11  5111.959961  5115.439941  5106.689941  5111.609863  5111.609863   \n",
       "\n",
       "                Volume  \n",
       "Date                    \n",
       "1978-01-03    17720000  \n",
       "1978-01-04    24090000  \n",
       "1978-01-05    23570000  \n",
       "1978-01-06    26150000  \n",
       "1978-01-09    27990000  \n",
       "...                ...  \n",
       "2024-03-05  4418410000  \n",
       "2024-03-06  4559050000  \n",
       "2024-03-07  4137980000  \n",
       "2024-03-08  4208870000  \n",
       "2024-03-11   167092004  \n",
       "\n",
       "[11646 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ml_df = yf.download('^GSPC', start=\"1978-01-01\", interval=\"1d\")\n",
    "ml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_regimes = pd.read_csv('sp500_regimes.csv')\n",
    "sp500_regimes.set_index('Date', inplace=True)\n",
    "sp500_regimes.index = pd.to_datetime(sp500_regimes.index)\n",
    "ml_df.index = pd.to_datetime(ml_df.index)\n",
    "data = pd.merge(ml_df.drop(columns=['High','Low','Open','Close','Adj Close']), sp500_regimes, on='Date',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['color'] = data['color'].shift(-1)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['color'])\n",
    "y = data['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10657, 10)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Assume y is your label data\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "y = to_categorical(y, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X and y are your data and labels\n",
    "train_size = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "# Assuming X is your dataset and y is the target variable\n",
    "oversampler = SMOTE()  # Choose your oversampler: RandomOverSampler, SMOTE, or ADASYN\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 10\n",
    "\n",
    "\n",
    "# Assuming you have X_resampled_reshaped and y_train as your training data\n",
    "train_size = int(len(X_resampled) * 0.8)  # 80% for training\n",
    "X_train, X_val = X_resampled[:train_size], X_resampled[train_size:]\n",
    "y_train, y_val = y_resampled[:train_size], y_resampled[train_size:]\n",
    "\n",
    "X_train= np.reshape(X_train,(X_train.shape[0], n_timesteps, X_train.shape[1]//n_timesteps))\n",
    "X_val = np.reshape(X_val,(X_val.shape[0], n_timesteps, X_val.shape[1]//n_timesteps))\n",
    "X_test_reshaped = np.reshape(X_test,(X_test.shape[0], n_timesteps, X_test.shape[1]//n_timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23468, 4)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "587/587 [==============================] - 4s 5ms/step - loss: 0.7627 - accuracy: 0.6574 - val_loss: 1.9081 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.4780 - accuracy: 0.8019 - val_loss: 1.6087 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.3650 - accuracy: 0.8564 - val_loss: 1.1227 - val_accuracy: 0.1536\n",
      "Epoch 4/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.3196 - accuracy: 0.8757 - val_loss: 1.1786 - val_accuracy: 0.2028\n",
      "Epoch 5/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.2400 - accuracy: 0.9112 - val_loss: 0.4263 - val_accuracy: 0.8057\n",
      "Epoch 6/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.1532 - accuracy: 0.9496 - val_loss: 0.1526 - val_accuracy: 0.9540\n",
      "Epoch 7/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.1143 - accuracy: 0.9630 - val_loss: 0.5586 - val_accuracy: 0.7744\n",
      "Epoch 8/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.1064 - accuracy: 0.9656 - val_loss: 0.1179 - val_accuracy: 0.9621\n",
      "Epoch 9/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0884 - accuracy: 0.9716 - val_loss: 0.2947 - val_accuracy: 0.8790\n",
      "Epoch 10/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0938 - accuracy: 0.9693 - val_loss: 0.2639 - val_accuracy: 0.9043\n",
      "Epoch 11/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0902 - accuracy: 0.9700 - val_loss: 0.1354 - val_accuracy: 0.9487\n",
      "Epoch 12/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0934 - accuracy: 0.9688 - val_loss: 0.3386 - val_accuracy: 0.8398\n",
      "Epoch 13/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0838 - accuracy: 0.9724 - val_loss: 0.2039 - val_accuracy: 0.9325\n",
      "Epoch 14/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0779 - accuracy: 0.9742 - val_loss: 0.1292 - val_accuracy: 0.9544\n",
      "Epoch 15/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0797 - accuracy: 0.9732 - val_loss: 0.1388 - val_accuracy: 0.9382\n",
      "Epoch 16/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0831 - accuracy: 0.9709 - val_loss: 0.1636 - val_accuracy: 0.9467\n",
      "Epoch 17/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0770 - accuracy: 0.9733 - val_loss: 0.0880 - val_accuracy: 0.9712\n",
      "Epoch 18/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0806 - accuracy: 0.9711 - val_loss: 0.1054 - val_accuracy: 0.9597\n",
      "Epoch 19/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0714 - accuracy: 0.9744 - val_loss: 0.0743 - val_accuracy: 0.9695\n",
      "Epoch 20/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0995 - accuracy: 0.9663 - val_loss: 0.0526 - val_accuracy: 0.9832\n",
      "Epoch 21/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0692 - accuracy: 0.9759 - val_loss: 0.1163 - val_accuracy: 0.9604\n",
      "Epoch 22/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0686 - accuracy: 0.9769 - val_loss: 0.0783 - val_accuracy: 0.9768\n",
      "Epoch 23/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0725 - accuracy: 0.9761 - val_loss: 0.1560 - val_accuracy: 0.9365\n",
      "Epoch 24/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0685 - accuracy: 0.9766 - val_loss: 0.0655 - val_accuracy: 0.9787\n",
      "Epoch 25/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0752 - accuracy: 0.9740 - val_loss: 0.0543 - val_accuracy: 0.9815\n",
      "Epoch 26/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0689 - accuracy: 0.9760 - val_loss: 0.1547 - val_accuracy: 0.9329\n",
      "Epoch 27/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0671 - accuracy: 0.9761 - val_loss: 0.0711 - val_accuracy: 0.9744\n",
      "Epoch 28/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0701 - accuracy: 0.9748 - val_loss: 0.3142 - val_accuracy: 0.8641\n",
      "Epoch 29/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0679 - accuracy: 0.9757 - val_loss: 0.2345 - val_accuracy: 0.9086\n",
      "Epoch 30/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0647 - accuracy: 0.9776 - val_loss: 0.0942 - val_accuracy: 0.9697\n",
      "Epoch 31/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0754 - accuracy: 0.9745 - val_loss: 0.0694 - val_accuracy: 0.9742\n",
      "Epoch 32/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0670 - accuracy: 0.9774 - val_loss: 0.0862 - val_accuracy: 0.9680\n",
      "Epoch 33/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0648 - accuracy: 0.9765 - val_loss: 0.0841 - val_accuracy: 0.9683\n",
      "Epoch 34/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0619 - accuracy: 0.9786 - val_loss: 0.1074 - val_accuracy: 0.9634\n",
      "Epoch 35/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0644 - accuracy: 0.9771 - val_loss: 0.1175 - val_accuracy: 0.9606\n",
      "Epoch 36/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0624 - accuracy: 0.9771 - val_loss: 0.0884 - val_accuracy: 0.9651\n",
      "Epoch 37/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0686 - accuracy: 0.9758 - val_loss: 0.0975 - val_accuracy: 0.9599\n",
      "Epoch 38/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0645 - accuracy: 0.9780 - val_loss: 0.1834 - val_accuracy: 0.9372\n",
      "Epoch 39/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0608 - accuracy: 0.9776 - val_loss: 0.0591 - val_accuracy: 0.9789\n",
      "Epoch 40/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0643 - accuracy: 0.9775 - val_loss: 0.1294 - val_accuracy: 0.9589\n",
      "Epoch 41/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0650 - accuracy: 0.9765 - val_loss: 0.1278 - val_accuracy: 0.9484\n",
      "Epoch 42/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0646 - accuracy: 0.9772 - val_loss: 0.2043 - val_accuracy: 0.9271\n",
      "Epoch 43/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0640 - accuracy: 0.9773 - val_loss: 0.2201 - val_accuracy: 0.9169\n",
      "Epoch 44/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0702 - accuracy: 0.9757 - val_loss: 0.1033 - val_accuracy: 0.9614\n",
      "Epoch 45/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0611 - accuracy: 0.9785 - val_loss: 0.1166 - val_accuracy: 0.9472\n",
      "Epoch 46/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0639 - accuracy: 0.9778 - val_loss: 0.0586 - val_accuracy: 0.9795\n",
      "Epoch 47/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0652 - accuracy: 0.9758 - val_loss: 0.1020 - val_accuracy: 0.9631\n",
      "Epoch 48/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0599 - accuracy: 0.9784 - val_loss: 0.2567 - val_accuracy: 0.8905\n",
      "Epoch 49/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0828 - accuracy: 0.9704 - val_loss: 0.0744 - val_accuracy: 0.9687\n",
      "Epoch 50/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0649 - accuracy: 0.9773 - val_loss: 0.0972 - val_accuracy: 0.9640\n",
      "Epoch 51/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0610 - accuracy: 0.9778 - val_loss: 0.0559 - val_accuracy: 0.9821\n",
      "Epoch 52/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0732 - accuracy: 0.9746 - val_loss: 0.0580 - val_accuracy: 0.9823\n",
      "Epoch 53/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0627 - accuracy: 0.9786 - val_loss: 0.1590 - val_accuracy: 0.9348\n",
      "Epoch 54/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0666 - accuracy: 0.9772 - val_loss: 0.1293 - val_accuracy: 0.9482\n",
      "Epoch 55/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0642 - accuracy: 0.9776 - val_loss: 0.1196 - val_accuracy: 0.9570\n",
      "Epoch 56/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0566 - accuracy: 0.9797 - val_loss: 0.0685 - val_accuracy: 0.9772\n",
      "Epoch 57/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0667 - accuracy: 0.9768 - val_loss: 0.0814 - val_accuracy: 0.9678\n",
      "Epoch 58/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0551 - accuracy: 0.9811 - val_loss: 0.0563 - val_accuracy: 0.9795\n",
      "Epoch 59/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0597 - accuracy: 0.9783 - val_loss: 0.1349 - val_accuracy: 0.9504\n",
      "Epoch 60/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.2485 - accuracy: 0.9109 - val_loss: 1.1387 - val_accuracy: 0.4020\n",
      "Epoch 61/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.2540 - accuracy: 0.9084 - val_loss: 0.3054 - val_accuracy: 0.8541\n",
      "Epoch 62/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.2152 - accuracy: 0.9205 - val_loss: 2.0957 - val_accuracy: 0.2804\n",
      "Epoch 63/100\n",
      "587/587 [==============================] - 3s 5ms/step - loss: 0.1200 - accuracy: 0.9564 - val_loss: 0.2371 - val_accuracy: 0.8926\n",
      "Epoch 64/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0795 - accuracy: 0.9725 - val_loss: 0.0962 - val_accuracy: 0.9700\n",
      "Epoch 65/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0611 - accuracy: 0.9798 - val_loss: 0.1929 - val_accuracy: 0.9139\n",
      "Epoch 66/100\n",
      "587/587 [==============================] - 41s 70ms/step - loss: 0.0955 - accuracy: 0.9684 - val_loss: 0.0971 - val_accuracy: 0.9657\n",
      "Epoch 67/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0574 - accuracy: 0.9801 - val_loss: 0.1004 - val_accuracy: 0.9617\n",
      "Epoch 68/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0551 - accuracy: 0.9810 - val_loss: 0.0987 - val_accuracy: 0.9685\n",
      "Epoch 69/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0712 - accuracy: 0.9749 - val_loss: 0.1161 - val_accuracy: 0.9570\n",
      "Epoch 70/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0623 - accuracy: 0.9781 - val_loss: 0.0727 - val_accuracy: 0.9757\n",
      "Epoch 71/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0611 - accuracy: 0.9787 - val_loss: 0.0660 - val_accuracy: 0.9719\n",
      "Epoch 72/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0636 - accuracy: 0.9775 - val_loss: 0.0903 - val_accuracy: 0.9651\n",
      "Epoch 73/100\n",
      "587/587 [==============================] - 46s 79ms/step - loss: 0.0669 - accuracy: 0.9759 - val_loss: 0.0791 - val_accuracy: 0.9702\n",
      "Epoch 74/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0547 - accuracy: 0.9802 - val_loss: 0.0997 - val_accuracy: 0.9610\n",
      "Epoch 75/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0704 - accuracy: 0.9759 - val_loss: 0.0759 - val_accuracy: 0.9695\n",
      "Epoch 76/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0670 - accuracy: 0.9760 - val_loss: 0.1090 - val_accuracy: 0.9576\n",
      "Epoch 77/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.1094 - accuracy: 0.9658 - val_loss: 0.1219 - val_accuracy: 0.9531\n",
      "Epoch 78/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0597 - accuracy: 0.9793 - val_loss: 0.0550 - val_accuracy: 0.9776\n",
      "Epoch 79/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0695 - accuracy: 0.9750 - val_loss: 0.1393 - val_accuracy: 0.9504\n",
      "Epoch 80/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0753 - accuracy: 0.9738 - val_loss: 0.2521 - val_accuracy: 0.8933\n",
      "Epoch 81/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0705 - accuracy: 0.9749 - val_loss: 0.0853 - val_accuracy: 0.9653\n",
      "Epoch 82/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.0563 - accuracy: 0.9795 - val_loss: 0.0578 - val_accuracy: 0.9806\n",
      "Epoch 83/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.2983 - accuracy: 0.8948 - val_loss: 1.0344 - val_accuracy: 0.2399\n",
      "Epoch 84/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.3041 - accuracy: 0.8776 - val_loss: 1.2928 - val_accuracy: 0.2235\n",
      "Epoch 85/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.2980 - accuracy: 0.8778 - val_loss: 1.1589 - val_accuracy: 0.2405\n",
      "Epoch 86/100\n",
      "587/587 [==============================] - 30s 51ms/step - loss: 0.2906 - accuracy: 0.8806 - val_loss: 1.6391 - val_accuracy: 0.2092\n",
      "Epoch 87/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.2867 - accuracy: 0.8814 - val_loss: 1.5139 - val_accuracy: 0.2175\n",
      "Epoch 88/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.2829 - accuracy: 0.8836 - val_loss: 1.4772 - val_accuracy: 0.2147\n",
      "Epoch 89/100\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.2861 - accuracy: 0.8812 - val_loss: 1.6209 - val_accuracy: 0.2079\n",
      "Epoch 90/100\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.2792 - accuracy: 0.8844 - val_loss: 1.0324 - val_accuracy: 0.2633\n",
      "Epoch 91/100\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.2773 - accuracy: 0.8855 - val_loss: 1.2209 - val_accuracy: 0.2448\n",
      "Epoch 92/100\n",
      "587/587 [==============================] - 234s 399ms/step - loss: 0.2743 - accuracy: 0.8865 - val_loss: 1.6401 - val_accuracy: 0.2096\n",
      "Epoch 93/100\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.2742 - accuracy: 0.8862 - val_loss: 1.3992 - val_accuracy: 0.2024\n",
      "Epoch 94/100\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.2752 - accuracy: 0.8847 - val_loss: 1.5054 - val_accuracy: 0.2030\n",
      "Epoch 95/100\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.2487 - accuracy: 0.9034 - val_loss: 1.5526 - val_accuracy: 0.2965\n",
      "Epoch 96/100\n",
      "587/587 [==============================] - 137s 234ms/step - loss: 0.1557 - accuracy: 0.9469 - val_loss: 0.8091 - val_accuracy: 0.6131\n",
      "Epoch 97/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.1256 - accuracy: 0.9588 - val_loss: 0.6276 - val_accuracy: 0.7137\n",
      "Epoch 98/100\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.1158 - accuracy: 0.9621 - val_loss: 0.1737 - val_accuracy: 0.9378\n",
      "Epoch 99/100\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.1452 - accuracy: 0.9492 - val_loss: 0.1134 - val_accuracy: 0.9704\n",
      "Epoch 100/100\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.2457 - accuracy: 0.9215 - val_loss: 1.9551 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3873ee990>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Assuming your data has 100 time steps and 10 features\n",
    "\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(83, input_shape=(10, 1)))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 82.59850144386292%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "\n",
    "print(f'Test accuracy: {test_accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |  neurons  |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8507   \u001b[0m | \u001b[0m0.004229 \u001b[0m | \u001b[0m74.83    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.2049   \u001b[0m | \u001b[0m0.0001011\u001b[0m | \u001b[0m37.21    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.02791  \u001b[0m | \u001b[0m0.001553 \u001b[0m | \u001b[0m18.31    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.5684   \u001b[0m | \u001b[0m0.001944 \u001b[0m | \u001b[0m41.1     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.9723   \u001b[0m | \u001b[95m0.004028 \u001b[0m | \u001b[95m58.49    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.9329   \u001b[0m | \u001b[0m0.00425  \u001b[0m | \u001b[0m71.67    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.873    \u001b[0m | \u001b[0m0.002124 \u001b[0m | \u001b[0m89.03    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m8        \u001b[0m | \u001b[95m0.9749   \u001b[0m | \u001b[95m0.0003711\u001b[0m | \u001b[95m70.34    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.9312   \u001b[0m | \u001b[0m0.004231 \u001b[0m | \u001b[0m60.28    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.4218   \u001b[0m | \u001b[0m0.00149  \u001b[0m | \u001b[0m27.83    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.2959   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m50.22    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.7825   \u001b[0m | \u001b[0m0.007369 \u001b[0m | \u001b[0m97.69    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m13       \u001b[0m | \u001b[95m0.9808   \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m82.54    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.5946   \u001b[0m | \u001b[0m0.008415 \u001b[0m | \u001b[0m65.74    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.912    \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m79.2     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.00125  \u001b[0m | \u001b[0m10.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.9427   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m93.29    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.9519   \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m85.41    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.2637   \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m56.03    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.6187   \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m45.17    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.9651   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m100.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.4182   \u001b[0m | \u001b[0m0.0007324\u001b[0m | \u001b[0m91.34    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.8291   \u001b[0m | \u001b[0m0.005667 \u001b[0m | \u001b[0m95.08    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.9504   \u001b[0m | \u001b[0m0.008463 \u001b[0m | \u001b[0m87.24    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.9222   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m77.09    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.4193   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m32.22    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.08351  \u001b[0m | \u001b[0m0.003049 \u001b[0m | \u001b[0m23.59    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.4604   \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m68.41    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.582    \u001b[0m | \u001b[0m0.001649 \u001b[0m | \u001b[0m62.82    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.9548   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m80.98    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.5315   \u001b[0m | \u001b[0m0.007515 \u001b[0m | \u001b[0m83.96    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.8664   \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m73.24    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m14.18    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.4717   \u001b[0m | \u001b[0m0.006027 \u001b[0m | \u001b[0m47.35    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.2224   \u001b[0m | \u001b[0m0.005819 \u001b[0m | \u001b[0m43.17    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.9659   \u001b[0m | \u001b[0m0.003256 \u001b[0m | \u001b[0m99.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.8862   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m59.34    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.6896   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m86.27    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.8641   \u001b[0m | \u001b[0m0.00107  \u001b[0m | \u001b[0m88.09    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.2561   \u001b[0m | \u001b[0m0.009478 \u001b[0m | \u001b[0m78.13    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.2258   \u001b[0m | \u001b[0m0.004727 \u001b[0m | \u001b[0m76.14    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.9644   \u001b[0m | \u001b[0m0.009986 \u001b[0m | \u001b[0m81.79    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.9431   \u001b[0m | \u001b[0m0.000941 \u001b[0m | \u001b[0m68.3     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.6723   \u001b[0m | \u001b[0m0.008963 \u001b[0m | \u001b[0m52.33    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.9233   \u001b[0m | \u001b[0m0.004378 \u001b[0m | \u001b[0m87.89    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.1176   \u001b[0m | \u001b[0m0.0003762\u001b[0m | \u001b[0m22.44    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.9595   \u001b[0m | \u001b[0m0.009267 \u001b[0m | \u001b[0m51.09    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.8963   \u001b[0m | \u001b[0m0.008835 \u001b[0m | \u001b[0m85.62    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.5315   \u001b[0m | \u001b[0m0.006151 \u001b[0m | \u001b[0m82.81    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.2397   \u001b[0m | \u001b[0m0.0007201\u001b[0m | \u001b[0m57.47    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.9521   \u001b[0m | \u001b[0m0.003498 \u001b[0m | \u001b[0m67.54    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.255    \u001b[0m | \u001b[0m0.007299 \u001b[0m | \u001b[0m31.73    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.2422   \u001b[0m | \u001b[0m0.00754  \u001b[0m | \u001b[0m87.29    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.5752   \u001b[0m | \u001b[0m0.001671 \u001b[0m | \u001b[0m70.28    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.9333   \u001b[0m | \u001b[0m0.002996 \u001b[0m | \u001b[0m99.8     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m56       \u001b[0m | \u001b[0m0.2009   \u001b[0m | \u001b[0m0.009587 \u001b[0m | \u001b[0m46.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m57       \u001b[0m | \u001b[0m0.9329   \u001b[0m | \u001b[0m0.0014   \u001b[0m | \u001b[0m87.76    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m58       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.006921 \u001b[0m | \u001b[0m11.48    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m59       \u001b[0m | \u001b[0m0.9139   \u001b[0m | \u001b[0m0.006773 \u001b[0m | \u001b[0m96.74    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m60       \u001b[0m | \u001b[0m0.601    \u001b[0m | \u001b[0m0.00202  \u001b[0m | \u001b[0m26.77    \u001b[0m |\n",
      "=================================================\n",
      "Iteration 0: \n",
      "\t{'target': 0.8506604433059692, 'params': {'lr': 0.004228517846555483, 'neurons': 74.82920440979423}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.2049424797296524, 'params': {'lr': 0.00010113231069171439, 'neurons': 37.20993153686558}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.027907967567443848, 'params': {'lr': 0.0015528833190894193, 'neurons': 18.310473529191803}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.5683851838111877, 'params': {'lr': 0.0019439760926389421, 'neurons': 41.1004654338743}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.9723050594329834, 'params': {'lr': 0.004027997994883633, 'neurons': 58.493506060302124}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.9328930377960205, 'params': {'lr': 0.004250025692592619, 'neurons': 71.66975503570836}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.8730294108390808, 'params': {'lr': 0.0021240772723420225, 'neurons': 89.03056927518509}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.9748615026473999, 'params': {'lr': 0.00037113717265946903, 'neurons': 70.3420759160562}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.9311887621879578, 'params': {'lr': 0.004231317543434558, 'neurons': 60.282084560117646}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.42181509733200073, 'params': {'lr': 0.0014898306920928144, 'neurons': 27.829134017639092}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.29590967297554016, 'params': {'lr': 0.01, 'neurons': 50.21660743991474}}\n",
      "Iteration 11: \n",
      "\t{'target': 0.7824882864952087, 'params': {'lr': 0.0073688252426756815, 'neurons': 97.69283005382887}}\n",
      "Iteration 12: \n",
      "\t{'target': 0.9808266162872314, 'params': {'lr': 0.01, 'neurons': 82.53670929046065}}\n",
      "Iteration 13: \n",
      "\t{'target': 0.5945888161659241, 'params': {'lr': 0.008415453917714383, 'neurons': 65.7433920610041}}\n",
      "Iteration 14: \n",
      "\t{'target': 0.9120153188705444, 'params': {'lr': 0.0001, 'neurons': 79.2036825595387}}\n",
      "Iteration 15: \n",
      "\t{'target': 0.0, 'params': {'lr': 0.0012499417323986234, 'neurons': 10.003336445750579}}\n",
      "Iteration 16: \n",
      "\t{'target': 0.9426928162574768, 'params': {'lr': 0.01, 'neurons': 93.29160701581286}}\n",
      "Iteration 17: \n",
      "\t{'target': 0.9518534541130066, 'params': {'lr': 0.0001, 'neurons': 85.41338760441437}}\n",
      "Iteration 18: \n",
      "\t{'target': 0.2637409567832947, 'params': {'lr': 0.0001, 'neurons': 56.026456155222604}}\n",
      "Iteration 19: \n",
      "\t{'target': 0.6186621189117432, 'params': {'lr': 0.0001, 'neurons': 45.1680038617553}}\n",
      "Iteration 20: \n",
      "\t{'target': 0.9650617837905884, 'params': {'lr': 0.01, 'neurons': 100.0}}\n",
      "Iteration 21: \n",
      "\t{'target': 0.41819342970848083, 'params': {'lr': 0.0007324306167610402, 'neurons': 91.33847743240868}}\n",
      "Iteration 22: \n",
      "\t{'target': 0.8291435837745667, 'params': {'lr': 0.005666952912940245, 'neurons': 95.08296165621206}}\n",
      "Iteration 23: \n",
      "\t{'target': 0.9503621459007263, 'params': {'lr': 0.008463161699097142, 'neurons': 87.23801502944626}}\n",
      "Iteration 24: \n",
      "\t{'target': 0.9222411513328552, 'params': {'lr': 0.01, 'neurons': 77.09240207562898}}\n",
      "Iteration 25: \n",
      "\t{'target': 0.41925862431526184, 'params': {'lr': 0.01, 'neurons': 32.22049735998279}}\n",
      "Iteration 26: \n",
      "\t{'target': 0.0835108682513237, 'params': {'lr': 0.0030487960557792843, 'neurons': 23.59116053172619}}\n",
      "Iteration 27: \n",
      "\t{'target': 0.46037495136260986, 'params': {'lr': 0.0001, 'neurons': 68.40791012893263}}\n",
      "Iteration 28: \n",
      "\t{'target': 0.5820196270942688, 'params': {'lr': 0.0016487535470502042, 'neurons': 62.819438925892115}}\n",
      "Iteration 29: \n",
      "\t{'target': 0.9548359513282776, 'params': {'lr': 0.01, 'neurons': 80.97976620809762}}\n",
      "Iteration 30: \n",
      "\t{'target': 0.5315296053886414, 'params': {'lr': 0.0075149953501155225, 'neurons': 83.95817143917012}}\n",
      "Iteration 31: \n",
      "\t{'target': 0.8664252161979675, 'params': {'lr': 0.0001, 'neurons': 73.24247789039927}}\n",
      "Iteration 32: \n",
      "\t{'target': 0.0, 'params': {'lr': 0.01, 'neurons': 14.180139114548892}}\n",
      "Iteration 33: \n",
      "\t{'target': 0.4716659486293793, 'params': {'lr': 0.006027208213284759, 'neurons': 47.354423385127795}}\n",
      "Iteration 34: \n",
      "\t{'target': 0.22241158783435822, 'params': {'lr': 0.005818849377734152, 'neurons': 43.16859623365677}}\n",
      "Iteration 35: \n",
      "\t{'target': 0.9659139513969421, 'params': {'lr': 0.0032557241764227876, 'neurons': 98.99761173284364}}\n",
      "Iteration 36: \n",
      "\t{'target': 0.8862377405166626, 'params': {'lr': 0.01, 'neurons': 59.34074255572428}}\n",
      "Iteration 37: \n",
      "\t{'target': 0.6896037459373474, 'params': {'lr': 0.01, 'neurons': 86.27412196590697}}\n",
      "Iteration 38: \n",
      "\t{'target': 0.8640817999839783, 'params': {'lr': 0.0010699488243148772, 'neurons': 88.08901624036523}}\n",
      "Iteration 39: \n",
      "\t{'target': 0.2560715675354004, 'params': {'lr': 0.009477700530761768, 'neurons': 78.12942206328913}}\n",
      "Iteration 40: \n",
      "\t{'target': 0.22582019865512848, 'params': {'lr': 0.004727241978323029, 'neurons': 76.13908232902116}}\n",
      "Iteration 41: \n",
      "\t{'target': 0.9644226431846619, 'params': {'lr': 0.009985736841850425, 'neurons': 81.78990616957036}}\n",
      "Iteration 42: \n",
      "\t{'target': 0.9431188702583313, 'params': {'lr': 0.0009410433545251562, 'neurons': 68.29743691998604}}\n",
      "Iteration 43: \n",
      "\t{'target': 0.6723476648330688, 'params': {'lr': 0.008962747213839424, 'neurons': 52.33493593738921}}\n",
      "Iteration 44: \n",
      "\t{'target': 0.9233063459396362, 'params': {'lr': 0.004378489084415893, 'neurons': 87.89333171118028}}\n",
      "Iteration 45: \n",
      "\t{'target': 0.11759693175554276, 'params': {'lr': 0.000376230102389927, 'neurons': 22.438456948088817}}\n",
      "Iteration 46: \n",
      "\t{'target': 0.9595227837562561, 'params': {'lr': 0.009267056524727745, 'neurons': 51.09463181617873}}\n",
      "Iteration 47: \n",
      "\t{'target': 0.8962505459785461, 'params': {'lr': 0.00883504552028873, 'neurons': 85.62432043592919}}\n",
      "Iteration 48: \n",
      "\t{'target': 0.5315296053886414, 'params': {'lr': 0.006150864100288201, 'neurons': 82.81147474858005}}\n",
      "Iteration 49: \n",
      "\t{'target': 0.23966765403747559, 'params': {'lr': 0.0007200642566409928, 'neurons': 57.466576996280985}}\n",
      "Iteration 50: \n",
      "\t{'target': 0.9520664811134338, 'params': {'lr': 0.0034984526241698074, 'neurons': 67.53558638391294}}\n",
      "Iteration 51: \n",
      "\t{'target': 0.2550064027309418, 'params': {'lr': 0.007299454233853184, 'neurons': 31.73215812279401}}\n",
      "Iteration 52: \n",
      "\t{'target': 0.24222411215305328, 'params': {'lr': 0.007539733016373235, 'neurons': 87.288840688283}}\n",
      "Iteration 53: \n",
      "\t{'target': 0.5752024054527283, 'params': {'lr': 0.0016707069276449319, 'neurons': 70.27539787892283}}\n",
      "Iteration 54: \n",
      "\t{'target': 0.9333191514015198, 'params': {'lr': 0.0029964712265420817, 'neurons': 99.80274833364845}}\n",
      "Iteration 55: \n",
      "\t{'target': 0.20089475810527802, 'params': {'lr': 0.009586568088629125, 'neurons': 46.00324069487697}}\n",
      "Iteration 56: \n",
      "\t{'target': 0.9328930377960205, 'params': {'lr': 0.0013996528470952016, 'neurons': 87.75589813065152}}\n",
      "Iteration 57: \n",
      "\t{'target': 0.0, 'params': {'lr': 0.0069211009065281315, 'neurons': 11.484862791079681}}\n",
      "Iteration 58: \n",
      "\t{'target': 0.9139326810836792, 'params': {'lr': 0.006773227315014592, 'neurons': 96.73559641821953}}\n",
      "Iteration 59: \n",
      "\t{'target': 0.6009799838066101, 'params': {'lr': 0.002020472057283465, 'neurons': 26.766713616404132}}\n",
      "{'target': 0.9808266162872314, 'params': {'lr': 0.01, 'neurons': 82.53670929046065}}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Define a function to build and compile the model\n",
    "def create_model(lr, neurons):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(int(neurons), input_shape=(10, 1)))\n",
    "    model.add(Dense(4, activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define a function for the hyperparameter optimization\n",
    "def optimize_model(lr, neurons):\n",
    "    model = create_model(lr, neurons)\n",
    "    # Assuming you have X_train, y_train as your training data\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    # Assuming you have X_val, y_val as your validation data\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return val_accuracy\n",
    "\n",
    "# Define the bounds of the hyperparameters to optimize\n",
    "bounds = {'lr': (0.0001, 0.01), 'neurons': (10, 100)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_model,\n",
    "    pbounds=bounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=50)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantfin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
