{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alden/quantfin/lib/python3.11/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ml_df = yf.download('^GSPC', start=\"1978-01-01\", interval=\"1d\")\n",
    "\n",
    "for i in [1, 2, 3, 5, 7, 14, 21]:\n",
    "    ml_df[f'Close_{i}_Value'] = ml_df['Adj Close'].pct_change(i)\n",
    "    ml_df[f'Volume_{i}_Value'] = ml_df['Volume'].pct_change(i)\n",
    "ml_df.dropna(inplace=True)\n",
    "for i in [3, 7, 14, 21]:\n",
    "    ml_df[f'Volt_{i}_Value'] = np.log(1 + ml_df['Close_1_Value']).rolling(i).std()\n",
    "ml_df.dropna(inplace=True)  \n",
    "\n",
    "sp500_regimes = pd.read_csv('../data/sp500_regimes.csv')\n",
    "sp500_regimes.set_index('Date', inplace=True)\n",
    "sp500_regimes.index = pd.to_datetime(sp500_regimes.index)\n",
    "ml_df.index = pd.to_datetime(ml_df.index)\n",
    "data = pd.merge(ml_df.drop(columns=['High','Low','Open','Close','Adj Close']), sp500_regimes, on='Date',how='inner')\n",
    "data['color'] = data['color'].shift(-1)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['color'])\n",
    "y = data['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()\n",
    "X_columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume `data` is your DataFrame\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on your data and then transform it\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# If you want to convert it back to a DataFrame\n",
    "X = pd.DataFrame(X, columns=X_columns)\n",
    "\n",
    "# Assume y is your label data\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X and y are your data and labels\n",
    "train_size = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "# Assuming X is your dataset and y is the target variable\n",
    "oversampler = SMOTE()  # Choose your oversampler: RandomOverSampler, SMOTE, or ADASYN\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 1\n",
    "\n",
    "\n",
    "# Assuming you have X_resampled_reshaped and y_train as your training data\n",
    "train_size = int(len(X_resampled) * 0.8)  # 80% for training\n",
    "X_train, X_val = X_resampled[:train_size], X_resampled[train_size:]\n",
    "y_train, y_val = y_resampled[:train_size], y_resampled[train_size:]\n",
    "\n",
    "X_train= np.reshape(X_train,(X_train.shape[0], n_timesteps, X_train.shape[1]//n_timesteps))\n",
    "X_val = np.reshape(X_val,(X_val.shape[0], n_timesteps, X_val.shape[1]//n_timesteps))\n",
    "X_test_reshaped = np.reshape(X_test,(X_test.shape[0], n_timesteps, X_test.shape[1]//n_timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19897, 1, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "199/199 [==============================] - 5s 12ms/step - loss: 0.7682 - accuracy: 0.7175 - val_loss: 1.3593 - val_accuracy: 0.3112\n",
      "Epoch 2/100\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.4686 - accuracy: 0.8241 - val_loss: 1.8337 - val_accuracy: 0.0454\n",
      "Epoch 3/100\n",
      "199/199 [==============================] - 3s 14ms/step - loss: 0.4984 - accuracy: 0.8273 - val_loss: 1.9018 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "199/199 [==============================] - 2s 12ms/step - loss: 0.5199 - accuracy: 0.8210 - val_loss: 1.1119 - val_accuracy: 0.3819\n",
      "Epoch 5/100\n",
      "199/199 [==============================] - 2s 12ms/step - loss: 0.4400 - accuracy: 0.8489 - val_loss: 0.8946 - val_accuracy: 0.5250\n",
      "Epoch 6/100\n",
      "199/199 [==============================] - 2s 12ms/step - loss: 0.4360 - accuracy: 0.8527 - val_loss: 0.8309 - val_accuracy: 0.5793\n",
      "Epoch 7/100\n",
      "199/199 [==============================] - 2s 11ms/step - loss: 0.4216 - accuracy: 0.8722 - val_loss: 0.9141 - val_accuracy: 0.5948\n",
      "Epoch 8/100\n",
      "199/199 [==============================] - 2s 11ms/step - loss: 0.3534 - accuracy: 0.8952 - val_loss: 0.8672 - val_accuracy: 0.7095\n",
      "Epoch 9/100\n",
      "199/199 [==============================] - 2s 12ms/step - loss: 0.3486 - accuracy: 0.9100 - val_loss: 0.6097 - val_accuracy: 0.8247\n",
      "Epoch 10/100\n",
      "199/199 [==============================] - 2s 11ms/step - loss: 0.3419 - accuracy: 0.9177 - val_loss: 0.3849 - val_accuracy: 0.8907\n",
      "Epoch 11/100\n",
      "199/199 [==============================] - 2s 11ms/step - loss: 0.2781 - accuracy: 0.9275 - val_loss: 0.4176 - val_accuracy: 0.9015\n",
      "Epoch 12/100\n",
      "199/199 [==============================] - 2s 12ms/step - loss: 0.2591 - accuracy: 0.9286 - val_loss: 0.1812 - val_accuracy: 0.9628\n",
      "Epoch 13/100\n",
      "199/199 [==============================] - 2s 11ms/step - loss: 0.2323 - accuracy: 0.9392 - val_loss: 0.3328 - val_accuracy: 0.9106\n",
      "Epoch 14/100\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.2331 - accuracy: 0.9383 - val_loss: 0.1647 - val_accuracy: 0.9652\n",
      "Epoch 15/100\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.2180 - accuracy: 0.9403 - val_loss: 0.6978 - val_accuracy: 0.7831\n",
      "Epoch 16/100\n",
      "199/199 [==============================] - 2s 11ms/step - loss: 0.2193 - accuracy: 0.9406 - val_loss: 0.1071 - val_accuracy: 0.9843\n",
      "Epoch 17/100\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.2124 - accuracy: 0.9426 - val_loss: 0.4029 - val_accuracy: 0.9081\n",
      "Epoch 18/100\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.2114 - accuracy: 0.9422 - val_loss: 0.1568 - val_accuracy: 0.9642\n",
      "Epoch 19/100\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.2950 - accuracy: 0.9235 - val_loss: 0.2169 - val_accuracy: 0.9556\n",
      "Epoch 20/100\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.2308 - accuracy: 0.9440 - val_loss: 0.1636 - val_accuracy: 0.9650\n",
      "Epoch 21/100\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.2147 - accuracy: 0.9458 - val_loss: 0.1673 - val_accuracy: 0.9642\n",
      "Epoch 22/100\n",
      "199/199 [==============================] - 2s 12ms/step - loss: 0.2150 - accuracy: 0.9448 - val_loss: 0.1584 - val_accuracy: 0.9731\n",
      "Epoch 23/100\n",
      "199/199 [==============================] - 2s 12ms/step - loss: 0.2031 - accuracy: 0.9484 - val_loss: 0.3046 - val_accuracy: 0.9108\n",
      "Epoch 24/100\n",
      "199/199 [==============================] - 2s 11ms/step - loss: 0.2107 - accuracy: 0.9455 - val_loss: 0.2478 - val_accuracy: 0.9435\n",
      "Epoch 25/100\n",
      "199/199 [==============================] - 3s 13ms/step - loss: 0.2396 - accuracy: 0.9423 - val_loss: 0.2361 - val_accuracy: 0.9491\n",
      "Epoch 26/100\n",
      "199/199 [==============================] - 2s 12ms/step - loss: 0.2191 - accuracy: 0.9463 - val_loss: 0.2481 - val_accuracy: 0.9487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x454928690>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "neurons = 587.8010199520079\n",
    "model.add(LSTM(64, activation='leaky_relu',input_shape=(1, X_train.shape[2]), return_sequences=True,kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
    "model.add(GaussianNoise(0.01))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(128, return_sequences=True, activation='leaky_relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(LSTM(128, return_sequences=True, activation='leaky_relu'))\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, activation='leaky_relu'))\n",
    "# # model.add(Dropout(0.3))\n",
    "# model.add(LSTM(16, return_sequences=True, activation='leaky_relu'))\n",
    "# model.add(LSTM(256, return_sequences=True, activation='leaky_relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(LSTM(256, return_sequences=True, activation='leaky_relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(LSTM(16, return_sequences=True, activation='leaky_relu'))\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(int(neurons), input_shape=(1, X_train.shape[2]), return_sequences=True, activation='leaky_relu'))\n",
    "# model.add(LSTM(int(neurons)//2, return_sequences=True, activation='leaky_relu'))\n",
    "# model.add(LSTM(int(neurons)//4, return_sequences=True, activation='leaky_relu'))\n",
    "# model.add(LSTM(int(neurons)//8, return_sequences=True, activation='leaky_relu'))\n",
    "# model.add(LSTM(int(neurons)//16, activation='leaky_relu'))\n",
    "# model.add(Dense(4, activation='softmax'))\n",
    "model.add(LSTM(256, activation='leaky_relu',kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.008326932120119833), metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=100,validation_data=(X_val, y_val),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.87295436859131%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "\n",
    "print(f'Test accuracy: {test_accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |  neurons  |\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9851   \u001b[0m | \u001b[0m0.004229 \u001b[0m | \u001b[0m773.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.8961   \u001b[0m | \u001b[0m0.0001011\u001b[0m | \u001b[0m398.9    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.5212   \u001b[0m | \u001b[0m0.001553 \u001b[0m | \u001b[0m210.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.9765   \u001b[0m | \u001b[0m0.001944 \u001b[0m | \u001b[0m437.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.8959   \u001b[0m | \u001b[0m0.004028 \u001b[0m | \u001b[0m610.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.9443   \u001b[0m | \u001b[0m0.00425  \u001b[0m | \u001b[0m742.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.9795   \u001b[0m | \u001b[0m0.002124 \u001b[0m | \u001b[0m914.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9636   \u001b[0m | \u001b[0m0.0003711\u001b[0m | \u001b[0m728.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.6987   \u001b[0m | \u001b[0m0.004231 \u001b[0m | \u001b[0m628.6    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.9516   \u001b[0m | \u001b[0m0.00149  \u001b[0m | \u001b[0m305.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9481   \u001b[0m | \u001b[0m0.002394 \u001b[0m | \u001b[0m771.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.9216   \u001b[0m | \u001b[0m0.004058 \u001b[0m | \u001b[0m784.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.9622   \u001b[0m | \u001b[0m0.005146 \u001b[0m | \u001b[0m926.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9202   \u001b[0m | \u001b[0m0.006619 \u001b[0m | \u001b[0m450.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.7628   \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m424.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.9552   \u001b[0m | \u001b[0m0.003011 \u001b[0m | \u001b[0m903.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.968    \u001b[0m | \u001b[0m0.004873 \u001b[0m | \u001b[0m715.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.9276   \u001b[0m | \u001b[0m0.006408 \u001b[0m | \u001b[0m702.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.8824   \u001b[0m | \u001b[0m0.004772 \u001b[0m | \u001b[0m319.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.8269   \u001b[0m | \u001b[0m0.0006485\u001b[0m | \u001b[0m292.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.9841   \u001b[0m | \u001b[0m0.00557  \u001b[0m | \u001b[0m939.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.9355   \u001b[0m | \u001b[0m0.007582 \u001b[0m | \u001b[0m950.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.9703   \u001b[0m | \u001b[0m0.0006534\u001b[0m | \u001b[0m888.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.8919   \u001b[0m | \u001b[0m0.007453 \u001b[0m | \u001b[0m876.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.9646   \u001b[0m | \u001b[0m0.009505 \u001b[0m | \u001b[0m968.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.9264   \u001b[0m | \u001b[0m0.003804 \u001b[0m | \u001b[0m980.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.9465   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m684.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.9477   \u001b[0m | \u001b[0m0.003491 \u001b[0m | \u001b[0m671.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.9534   \u001b[0m | \u001b[0m0.008716 \u001b[0m | \u001b[0m658.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.9007   \u001b[0m | \u001b[0m0.006705 \u001b[0m | \u001b[0m999.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.8768   \u001b[0m | \u001b[0m0.000263 \u001b[0m | \u001b[0m527.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.6961   \u001b[0m | \u001b[0m0.009217 \u001b[0m | \u001b[0m475.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m33       \u001b[0m | \u001b[95m0.9867   \u001b[0m | \u001b[95m0.008327 \u001b[0m | \u001b[95m587.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.9672   \u001b[0m | \u001b[0m0.009856 \u001b[0m | \u001b[0m577.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.9614   \u001b[0m | \u001b[0m0.004764 \u001b[0m | \u001b[0m562.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.9684   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m549.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.954    \u001b[0m | \u001b[0m0.009686 \u001b[0m | \u001b[0m597.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.6046   \u001b[0m | \u001b[0m0.002653 \u001b[0m | \u001b[0m128.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.8277   \u001b[0m | \u001b[0m0.004834 \u001b[0m | \u001b[0m366.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.9729   \u001b[0m | \u001b[0m0.001395 \u001b[0m | \u001b[0m832.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.8517   \u001b[0m | \u001b[0m0.003224 \u001b[0m | \u001b[0m817.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.9083   \u001b[0m | \u001b[0m0.007305 \u001b[0m | \u001b[0m847.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.9524   \u001b[0m | \u001b[0m0.006175 \u001b[0m | \u001b[0m1.024e+03\u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.8995   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m255.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.6595   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m168.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.5855   \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m341.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.9198   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m505.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.7405   \u001b[0m | \u001b[0m0.005068 \u001b[0m | \u001b[0m383.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.6509   \u001b[0m | \u001b[0m0.008378 \u001b[0m | \u001b[0m270.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.8784   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m240.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.9694   \u001b[0m | \u001b[0m0.0001   \u001b[0m | \u001b[0m1.013e+03\u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.96     \u001b[0m | \u001b[0m0.006291 \u001b[0m | \u001b[0m861.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.9009   \u001b[0m | \u001b[0m0.003754 \u001b[0m | \u001b[0m539.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.9361   \u001b[0m | \u001b[0m0.004293 \u001b[0m | \u001b[0m647.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.9479   \u001b[0m | \u001b[0m0.007363 \u001b[0m | \u001b[0m801.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m56       \u001b[0m | \u001b[0m0.9594   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m494.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m57       \u001b[0m | \u001b[0m0.956    \u001b[0m | \u001b[0m0.006691 \u001b[0m | \u001b[0m752.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m58       \u001b[0m | \u001b[0m0.9102   \u001b[0m | \u001b[0m0.007406 \u001b[0m | \u001b[0m960.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m59       \u001b[0m | \u001b[0m0.9672   \u001b[0m | \u001b[0m0.0009342\u001b[0m | \u001b[0m693.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m60       \u001b[0m | \u001b[0m0.9087   \u001b[0m | \u001b[0m0.002737 \u001b[0m | \u001b[0m895.5    \u001b[0m |\n",
      "=================================================\n",
      "Iteration 0: \n",
      "\t{'target': 0.9851256012916565, 'params': {'lr': 0.004228517846555483, 'neurons': 773.4107461241737}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.8960803747177124, 'params': {'lr': 0.00010113231069171439, 'neurons': 398.88998507812846}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.5212060213088989, 'params': {'lr': 0.0015528833190894193, 'neurons': 210.73538091284283}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.9764823913574219, 'params': {'lr': 0.0019439760926389421, 'neurons': 437.62241143057076}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.8958793878555298, 'params': {'lr': 0.004027997994883633, 'neurons': 610.7797936670079}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.9443216323852539, 'params': {'lr': 0.004250025692592619, 'neurons': 741.9566723554965}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.979497492313385, 'params': {'lr': 0.0021240772723420225, 'neurons': 914.7932230062871}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.9636180996894836, 'params': {'lr': 0.00037113717265946903, 'neurons': 728.7388891198484}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.6986934542655945, 'params': {'lr': 0.004231317543434558, 'neurons': 628.5860862873935}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.9515578150749207, 'params': {'lr': 0.0014898306920928144, 'neurons': 305.4989342200514}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.9481406807899475, 'params': {'lr': 0.0023938826548400125, 'neurons': 771.8704590508427}}\n",
      "Iteration 11: \n",
      "\t{'target': 0.921608030796051, 'params': {'lr': 0.004057905574651423, 'neurons': 784.5226702738831}}\n",
      "Iteration 12: \n",
      "\t{'target': 0.9622110724449158, 'params': {'lr': 0.005145522705505821, 'neurons': 926.7310534122586}}\n",
      "Iteration 13: \n",
      "\t{'target': 0.9202010035514832, 'params': {'lr': 0.0066190392777323035, 'neurons': 450.4526177237986}}\n",
      "Iteration 14: \n",
      "\t{'target': 0.7628140449523926, 'params': {'lr': 0.0001, 'neurons': 424.71165018069865}}\n",
      "Iteration 15: \n",
      "\t{'target': 0.9551758766174316, 'params': {'lr': 0.0030114858138002272, 'neurons': 903.2988514963502}}\n",
      "Iteration 16: \n",
      "\t{'target': 0.9680402278900146, 'params': {'lr': 0.004873129656766928, 'neurons': 715.8762908476667}}\n",
      "Iteration 17: \n",
      "\t{'target': 0.9276381731033325, 'params': {'lr': 0.006407671428681487, 'neurons': 702.7118784124037}}\n",
      "Iteration 18: \n",
      "\t{'target': 0.8824120759963989, 'params': {'lr': 0.004771935436038639, 'neurons': 319.3873336595276}}\n",
      "Iteration 19: \n",
      "\t{'target': 0.8269346952438354, 'params': {'lr': 0.0006485469673377298, 'neurons': 292.3335479454645}}\n",
      "Iteration 20: \n",
      "\t{'target': 0.9841206073760986, 'params': {'lr': 0.005570352725934867, 'neurons': 939.3643073551921}}\n",
      "Iteration 21: \n",
      "\t{'target': 0.9354773759841919, 'params': {'lr': 0.007582044333514486, 'neurons': 950.5590657039}}\n",
      "Iteration 22: \n",
      "\t{'target': 0.9702512621879578, 'params': {'lr': 0.0006533577364974904, 'neurons': 888.9275442602967}}\n",
      "Iteration 23: \n",
      "\t{'target': 0.8918592929840088, 'params': {'lr': 0.007452920627384293, 'neurons': 876.6823537194326}}\n",
      "Iteration 24: \n",
      "\t{'target': 0.9646230936050415, 'params': {'lr': 0.009504840168268728, 'neurons': 968.353194221686}}\n",
      "Iteration 25: \n",
      "\t{'target': 0.9264321327209473, 'params': {'lr': 0.003803843555814341, 'neurons': 980.225673634659}}\n",
      "Iteration 26: \n",
      "\t{'target': 0.946532666683197, 'params': {'lr': 0.01, 'neurons': 684.629060337418}}\n",
      "Iteration 27: \n",
      "\t{'target': 0.9477387070655823, 'params': {'lr': 0.003490875756319394, 'neurons': 671.3984382457286}}\n",
      "Iteration 28: \n",
      "\t{'target': 0.9533668160438538, 'params': {'lr': 0.008715601164498071, 'neurons': 658.3439800005465}}\n",
      "Iteration 29: \n",
      "\t{'target': 0.900703489780426, 'params': {'lr': 0.006704945703409133, 'neurons': 999.4995195359697}}\n",
      "Iteration 30: \n",
      "\t{'target': 0.8767839074134827, 'params': {'lr': 0.00026303433711439894, 'neurons': 527.2129903666952}}\n",
      "Iteration 31: \n",
      "\t{'target': 0.6960803866386414, 'params': {'lr': 0.009217014848778595, 'neurons': 475.77287214418254}}\n",
      "Iteration 32: \n",
      "\t{'target': 0.9867336750030518, 'params': {'lr': 0.008326932120119833, 'neurons': 587.8010199520079}}\n",
      "Iteration 33: \n",
      "\t{'target': 0.9672361612319946, 'params': {'lr': 0.009856401725109924, 'neurons': 577.0398388033083}}\n",
      "Iteration 34: \n",
      "\t{'target': 0.9614070057868958, 'params': {'lr': 0.004763641098710152, 'neurons': 562.6884596660701}}\n",
      "Iteration 35: \n",
      "\t{'target': 0.9684422016143799, 'params': {'lr': 0.01, 'neurons': 549.5240787640088}}\n",
      "Iteration 36: \n",
      "\t{'target': 0.9539698362350464, 'params': {'lr': 0.009685732025775445, 'neurons': 597.5866735213935}}\n",
      "Iteration 37: \n",
      "\t{'target': 0.6046231389045715, 'params': {'lr': 0.0026528981863601737, 'neurons': 128.41790929051263}}\n",
      "Iteration 38: \n",
      "\t{'target': 0.8277387022972107, 'params': {'lr': 0.004833918895498631, 'neurons': 366.39073711731015}}\n",
      "Iteration 39: \n",
      "\t{'target': 0.9728643298149109, 'params': {'lr': 0.0013945304206641768, 'neurons': 832.6617064168419}}\n",
      "Iteration 40: \n",
      "\t{'target': 0.851658284664154, 'params': {'lr': 0.003224007664448697, 'neurons': 817.3685237519496}}\n",
      "Iteration 41: \n",
      "\t{'target': 0.9083417057991028, 'params': {'lr': 0.0073051402885746285, 'neurons': 846.9954149662954}}\n",
      "Iteration 42: \n",
      "\t{'target': 0.9523618221282959, 'params': {'lr': 0.006175205643286246, 'neurons': 1023.9265273644288}}\n",
      "Iteration 43: \n",
      "\t{'target': 0.8994975090026855, 'params': {'lr': 0.01, 'neurons': 255.08801614693257}}\n",
      "Iteration 44: \n",
      "\t{'target': 0.6594974994659424, 'params': {'lr': 0.01, 'neurons': 168.9009918091276}}\n",
      "Iteration 45: \n",
      "\t{'target': 0.5855276584625244, 'params': {'lr': 0.0001, 'neurons': 341.90050450578894}}\n",
      "Iteration 46: \n",
      "\t{'target': 0.9197989702224731, 'params': {'lr': 0.01, 'neurons': 505.78958943330446}}\n",
      "Iteration 47: \n",
      "\t{'target': 0.7405025362968445, 'params': {'lr': 0.005068293103582398, 'neurons': 383.9160101447808}}\n",
      "Iteration 48: \n",
      "\t{'target': 0.6508542895317078, 'params': {'lr': 0.00837775354166246, 'neurons': 270.8553347291627}}\n",
      "Iteration 49: \n",
      "\t{'target': 0.8783919811248779, 'params': {'lr': 0.01, 'neurons': 240.56611123959897}}\n",
      "Iteration 50: \n",
      "\t{'target': 0.9694472551345825, 'params': {'lr': 0.0001, 'neurons': 1012.8083335966086}}\n",
      "Iteration 51: \n",
      "\t{'target': 0.9599999785423279, 'params': {'lr': 0.006291317958237226, 'neurons': 861.2544734023393}}\n",
      "Iteration 52: \n",
      "\t{'target': 0.9009045362472534, 'params': {'lr': 0.0037543086472448515, 'neurons': 539.8562122426562}}\n",
      "Iteration 53: \n",
      "\t{'target': 0.9360803961753845, 'params': {'lr': 0.004293413268459286, 'neurons': 647.9260083296189}}\n",
      "Iteration 54: \n",
      "\t{'target': 0.9479396939277649, 'params': {'lr': 0.007362885789206263, 'neurons': 801.1070679162383}}\n",
      "Iteration 55: \n",
      "\t{'target': 0.9593969583511353, 'params': {'lr': 0.01, 'neurons': 494.6798627734966}}\n",
      "Iteration 56: \n",
      "\t{'target': 0.9559798836708069, 'params': {'lr': 0.006691334662040144, 'neurons': 752.9127129864247}}\n",
      "Iteration 57: \n",
      "\t{'target': 0.9101507663726807, 'params': {'lr': 0.007405586621648753, 'neurons': 960.2929885670493}}\n",
      "Iteration 58: \n",
      "\t{'target': 0.9672361612319946, 'params': {'lr': 0.0009342417021329114, 'neurons': 693.259519659462}}\n",
      "Iteration 59: \n",
      "\t{'target': 0.9087437391281128, 'params': {'lr': 0.002736618042070443, 'neurons': 895.522260140589}}\n",
      "{'target': 0.9867336750030518, 'params': {'lr': 0.008326932120119833, 'neurons': 587.8010199520079}}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    " \n",
    "# Define a function to build and compile the model\n",
    "\n",
    "def create_model(lr, neurons):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(int(neurons), input_shape=(1, X_train.shape[2]), return_sequences=True, activation='leaky_relu'))\n",
    "    model.add(LSTM(int(neurons)//2, return_sequences=True, activation='leaky_relu'))\n",
    "    model.add(LSTM(int(neurons)//4, return_sequences=True, activation='leaky_relu'))\n",
    "    model.add(LSTM(int(neurons)//8, return_sequences=True, activation='leaky_relu'))\n",
    "    model.add(LSTM(int(neurons)//16, activation='leaky_relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=lr), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define a function for the hyperparameter optimization\n",
    "def optimize_model(lr, neurons):\n",
    "    model = create_model(lr, neurons)\n",
    "    # Assuming you have X_train, y_train as your training data\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=128, verbose=0)\n",
    "    # Assuming you have X_val, y_val as your validation data\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return val_accuracy\n",
    "\n",
    "# Define the bounds of the hyperparameters to optimize\n",
    "bounds = {'lr': (0.0001, 0.01), 'neurons': (128, 1024)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_model,\n",
    "    pbounds=bounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=50)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 2ms/step\n",
      "Accuracy: 91.87%\n",
      "F1 Score: 0.9072968676964156\n",
      "67/67 [==============================] - 0s 2ms/step\n",
      "AUC-ROC (One-vs-Rest): 0.9733614320055951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' if you have a multi-class problem\n",
    "print(f'F1 Score: {f1}')\n",
    "y_prob_pred = model.predict(X_test_reshaped)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_prob_pred, multi_class='ovr')\n",
    "print(f'AUC-ROC (One-vs-Rest): {auc_roc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantfin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
